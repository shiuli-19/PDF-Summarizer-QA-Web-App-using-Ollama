## ğŸ“Œ Features

- **ğŸ“„ PDF Text Extraction:** Extracts raw text from PDFs using **PyMuPDF (fitz)**.
- **ğŸ§¾ OCR Fallback:** Uses **Tesseract OCR** for scanned PDFs when direct text extraction fails.
- **ğŸ§  Intelligent Chunking:** Breaks long documents into overlapping text chunks using `RecursiveCharacterTextSplitter` for better context.
- **ğŸ” FAISS Indexing:** Stores vector embeddings of chunks in a **FAISS** index for fast similarity search.
- **ğŸ§¬ Ollama Embeddings:** Uses local Ollama models like `nomic-embed-text` to generate embeddings for both PDF chunks and user queries.
- **ğŸ’¬ Context-Aware Q&A:** Answers user questions based on the top-k relevant chunks using Ollama LLMs (e.g., `llama3.2`) via prompt templating.
- **ğŸ“‘ Multi-mode Summarization:** Supports structured, bullet, and executive summaries using the same RAG + LLM pipeline.
- **ğŸ“¶ Ollama Connectivity Checks:** Confirms Ollama is up and models are available.
- **ğŸ©º System Status Reporting:** `/status` endpoint to check loaded PDFs, chunk count, and OCR/model health.

## ğŸ§ª Tech Stack

- **Flask** â€“ RESTful web backend
- **PyMuPDF** â€“ PDF parsing
- **PyTesseract** â€“ OCR fallback for scanned PDFs
- **FAISS** â€“ Vector indexing for semantic retrieval
- **Langchain/TextSplitters** â€“ Chunking and overlap control
- **Ollama** â€“ Local embeddings + LLM responses

## ğŸ¤– Models Used

- **nomic-embed-text** â†’ Generates vector embeddings for chunks and queries
- **llama3.2** â†’ Generates answers and summaries from relevant document chunks
