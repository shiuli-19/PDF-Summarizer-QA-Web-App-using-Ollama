# ğŸ“š PDF-Summarizer-QA-Web-App-using-Ollama

A Flask-based web application that lets you **upload PDFs**, **generate intelligent summaries**, and **ask context-aware questions** using local **Ollama** models with embeddings and LLMs. It uses **FAISS** for semantic search and supports scanned PDFs via **OCR** fallback.

---

## ğŸ“Œ Features

- **ğŸ“„ PDF Text Extraction:** Extracts raw text from PDFs using **PyMuPDF (fitz)**.
- **ğŸ§¾ OCR Fallback:** Uses **Tesseract OCR** for scanned PDFs when direct text extraction fails.
- **ğŸ§  Intelligent Chunking:** Breaks long documents into overlapping text chunks using `RecursiveCharacterTextSplitter` for better context.
- **ğŸ” FAISS Indexing:** Stores vector embeddings of chunks in a **FAISS** index for fast similarity search.
- **ğŸ§¬ Ollama Embeddings:** Uses local Ollama models like `nomic-embed-text` to generate embeddings for both PDF chunks and user queries.
- **ğŸ’¬ Context-Aware Q&A:** Answers user questions based on the top-k relevant chunks using Ollama LLMs (e.g., `llama3.2`) via prompt templating.
- **ğŸ“‘ Multi-mode Summarization:** Supports structured, bullet, and executive summaries using the same RAG + LLM pipeline.
- **ğŸ“¶ Ollama Connectivity Checks:** Confirms Ollama is up and models are available.
- **ğŸ©º System Status Reporting:** `/status` endpoint to check loaded PDFs, chunk count, and OCR/model health.

---

## ğŸ§ª Tech Stack

- **Flask** â€“ RESTful web backend
- **PyMuPDF** â€“ PDF parsing
- **PyTesseract** â€“ OCR fallback for scanned PDFs
- **FAISS** â€“ Vector indexing for semantic retrieval
- **Langchain/TextSplitters** â€“ Chunking and overlap control
- **Ollama** â€“ Local embeddings + LLM responses

---

## ğŸ¤– Models Used

- `nomic-embed-text` â†’ Generates vector embeddings for chunks and queries
- `llama3.2` â†’ Generates answers and summaries from relevant document chunks

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Prerequisites

- Python 3.8+
- [Tesseract OCR](https://github.com/tesseract-ocr/tesseract)
- [Ollama](https://ollama.com) running locally at `http://localhost:11434`
  - Models:
    - `ollama pull nomic-embed-text`
    - `ollama pull llama3.2`

---

### 2ï¸âƒ£ Installation

Clone the repo:

```bash
git clone https://github.com/shiuli-19/PDF-Summarizer-QA-Web-App-using-Ollama.git
cd PDF-Summarizer-QA-Web-App-using-Ollama
